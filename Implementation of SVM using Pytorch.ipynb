{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd965ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e47bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seed\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448c335",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f671f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f562548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary_SVM(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(Binary_SVM, self).__init__()\n",
    "        self.linear = nn.Linear(n_features, 1)\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x) # (N x 1)\n",
    "        out = out.squeeze() #(N)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987667b4",
   "metadata": {},
   "source": [
    "# Get Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a697faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b84d8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create pandas dataframe\n",
    "\n",
    "df = pd.DataFrame(data[\"data\"],columns = data[\"feature_names\"])\n",
    "df['target'] = data['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a026d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Class Name\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa33aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train-test-split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test =train_test_split(data['data'], 2 * data['target'] -1,test_size= 0.2, random_state = seed )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61544f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tensor dataset and dataloader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(x_train), torch.tensor(y_train))\n",
    "test_dataset= TensorDataset(torch.tensor(x_test), torch.tensor(y_test))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size =32, shuffle= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3529644a",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14e3c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Model\n",
    "\n",
    "model = Binary_SVM(x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1267864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters and Optimizer\n",
    "epochs = 1000\n",
    "C = 1.0\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a407085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0\t Loss : 0.278997540473938\n",
      "Epoch : 1\t Loss : 0.28406858444213867\n",
      "Epoch : 2\t Loss : 0.2715037763118744\n",
      "Epoch : 3\t Loss : 0.2639671266078949\n",
      "Epoch : 4\t Loss : 0.27572014927864075\n",
      "Epoch : 5\t Loss : 0.2733612358570099\n",
      "Epoch : 6\t Loss : 0.2775592505931854\n",
      "Epoch : 7\t Loss : 0.28036683797836304\n",
      "Epoch : 8\t Loss : 0.27643319964408875\n",
      "Epoch : 9\t Loss : 0.2736806869506836\n",
      "Epoch : 10\t Loss : 0.27055612206459045\n",
      "Epoch : 11\t Loss : 0.26869359612464905\n",
      "Epoch : 12\t Loss : 0.26622143387794495\n",
      "Epoch : 13\t Loss : 0.26533424854278564\n",
      "Epoch : 14\t Loss : 0.26436614990234375\n",
      "Epoch : 15\t Loss : 0.2625672221183777\n",
      "Epoch : 16\t Loss : 0.2632630169391632\n",
      "Epoch : 17\t Loss : 0.2621096074581146\n",
      "Epoch : 18\t Loss : 0.2603335678577423\n",
      "Epoch : 19\t Loss : 0.26129671931266785\n",
      "Epoch : 20\t Loss : 0.26218563318252563\n",
      "Epoch : 21\t Loss : 0.26038989424705505\n",
      "Epoch : 22\t Loss : 0.26135748624801636\n",
      "Epoch : 23\t Loss : 0.2593098282814026\n",
      "Epoch : 24\t Loss : 0.2593553960323334\n",
      "Epoch : 25\t Loss : 0.260841965675354\n",
      "Epoch : 26\t Loss : 0.2637151777744293\n",
      "Epoch : 27\t Loss : 0.26298266649246216\n",
      "Epoch : 28\t Loss : 0.2635556161403656\n",
      "Epoch : 29\t Loss : 0.2633231282234192\n",
      "Epoch : 30\t Loss : 0.26260969042778015\n",
      "Epoch : 31\t Loss : 0.2610716223716736\n",
      "Epoch : 32\t Loss : 0.26095902919769287\n",
      "Epoch : 33\t Loss : 0.26200926303863525\n",
      "Epoch : 34\t Loss : 0.26102709770202637\n",
      "Epoch : 35\t Loss : 0.25964051485061646\n",
      "Epoch : 36\t Loss : 0.25933364033699036\n",
      "Epoch : 37\t Loss : 0.2584846317768097\n",
      "Epoch : 38\t Loss : 0.25729072093963623\n",
      "Epoch : 39\t Loss : 0.25592857599258423\n",
      "Epoch : 40\t Loss : 0.25472119450569153\n",
      "Epoch : 41\t Loss : 0.2539385259151459\n",
      "Epoch : 42\t Loss : 0.2532883882522583\n",
      "Epoch : 43\t Loss : 0.2532724440097809\n",
      "Epoch : 44\t Loss : 0.25250008702278137\n",
      "Epoch : 45\t Loss : 0.2513485252857208\n",
      "Epoch : 46\t Loss : 0.2502710521221161\n",
      "Epoch : 47\t Loss : 0.249188631772995\n",
      "Epoch : 48\t Loss : 0.24978594481945038\n",
      "Epoch : 49\t Loss : 0.2490471601486206\n",
      "Epoch : 50\t Loss : 0.24868781864643097\n",
      "Epoch : 51\t Loss : 0.24840857088565826\n",
      "Epoch : 52\t Loss : 0.24800997972488403\n",
      "Epoch : 53\t Loss : 0.24753974378108978\n",
      "Epoch : 54\t Loss : 0.24684178829193115\n",
      "Epoch : 55\t Loss : 0.24602706730365753\n",
      "Epoch : 56\t Loss : 0.24525867402553558\n",
      "Epoch : 57\t Loss : 0.24458259344100952\n",
      "Epoch : 58\t Loss : 0.24429276585578918\n",
      "Epoch : 59\t Loss : 0.24348194897174835\n",
      "Epoch : 60\t Loss : 0.24323834478855133\n",
      "Epoch : 61\t Loss : 0.2437712848186493\n",
      "Epoch : 62\t Loss : 0.24461700022220612\n",
      "Epoch : 63\t Loss : 0.2440357804298401\n",
      "Epoch : 64\t Loss : 0.2433176040649414\n",
      "Epoch : 65\t Loss : 0.2429700791835785\n",
      "Epoch : 66\t Loss : 0.24299517273902893\n",
      "Epoch : 67\t Loss : 0.2422996312379837\n",
      "Epoch : 68\t Loss : 0.2414885014295578\n",
      "Epoch : 69\t Loss : 0.24115943908691406\n",
      "Epoch : 70\t Loss : 0.24249029159545898\n",
      "Epoch : 71\t Loss : 0.2424667328596115\n",
      "Epoch : 72\t Loss : 0.24185286462306976\n",
      "Epoch : 73\t Loss : 0.24188955128192902\n",
      "Epoch : 74\t Loss : 0.24114640057086945\n",
      "Epoch : 75\t Loss : 0.24051786959171295\n",
      "Epoch : 76\t Loss : 0.24014538526535034\n",
      "Epoch : 77\t Loss : 0.2403777837753296\n",
      "Epoch : 78\t Loss : 0.239756241440773\n",
      "Epoch : 79\t Loss : 0.23920069634914398\n",
      "Epoch : 80\t Loss : 0.23885869979858398\n",
      "Epoch : 81\t Loss : 0.23817746341228485\n",
      "Epoch : 82\t Loss : 0.23816077411174774\n",
      "Epoch : 83\t Loss : 0.23808082938194275\n",
      "Epoch : 84\t Loss : 0.2375553399324417\n",
      "Epoch : 85\t Loss : 0.23698848485946655\n",
      "Epoch : 86\t Loss : 0.23637241125106812\n",
      "Epoch : 87\t Loss : 0.23568785190582275\n",
      "Epoch : 88\t Loss : 0.2351568192243576\n",
      "Epoch : 89\t Loss : 0.2346261888742447\n",
      "Epoch : 90\t Loss : 0.23418612778186798\n",
      "Epoch : 91\t Loss : 0.2337743639945984\n",
      "Epoch : 92\t Loss : 0.233468160033226\n",
      "Epoch : 93\t Loss : 0.23308411240577698\n",
      "Epoch : 94\t Loss : 0.23271259665489197\n",
      "Epoch : 95\t Loss : 0.23238605260849\n",
      "Epoch : 96\t Loss : 0.23180565237998962\n",
      "Epoch : 97\t Loss : 0.23139247298240662\n",
      "Epoch : 98\t Loss : 0.2311234474182129\n",
      "Epoch : 99\t Loss : 0.23061303794384003\n",
      "Epoch : 100\t Loss : 0.23018835484981537\n",
      "Epoch : 101\t Loss : 0.2300192415714264\n",
      "Epoch : 102\t Loss : 0.22966952621936798\n",
      "Epoch : 103\t Loss : 0.22921520471572876\n",
      "Epoch : 104\t Loss : 0.22891759872436523\n",
      "Epoch : 105\t Loss : 0.22850118577480316\n",
      "Epoch : 106\t Loss : 0.2279895395040512\n",
      "Epoch : 107\t Loss : 0.22762538492679596\n",
      "Epoch : 108\t Loss : 0.22779348492622375\n",
      "Epoch : 109\t Loss : 0.227267786860466\n",
      "Epoch : 110\t Loss : 0.22697816789150238\n",
      "Epoch : 111\t Loss : 0.2265990525484085\n",
      "Epoch : 112\t Loss : 0.2261110544204712\n",
      "Epoch : 113\t Loss : 0.22563454508781433\n",
      "Epoch : 114\t Loss : 0.2253548949956894\n",
      "Epoch : 115\t Loss : 0.22545300424098969\n",
      "Epoch : 116\t Loss : 0.22526220977306366\n",
      "Epoch : 117\t Loss : 0.22500471770763397\n",
      "Epoch : 118\t Loss : 0.22467857599258423\n",
      "Epoch : 119\t Loss : 0.22422929108142853\n",
      "Epoch : 120\t Loss : 0.22402231395244598\n",
      "Epoch : 121\t Loss : 0.22361893951892853\n",
      "Epoch : 122\t Loss : 0.22327779233455658\n",
      "Epoch : 123\t Loss : 0.2229246199131012\n",
      "Epoch : 124\t Loss : 0.22258827090263367\n",
      "Epoch : 125\t Loss : 0.2226240485906601\n",
      "Epoch : 126\t Loss : 0.2220950722694397\n",
      "Epoch : 127\t Loss : 0.22251716256141663\n",
      "Epoch : 128\t Loss : 0.22321614623069763\n",
      "Epoch : 129\t Loss : 0.22314134240150452\n",
      "Epoch : 130\t Loss : 0.22270837426185608\n",
      "Epoch : 131\t Loss : 0.22272850573062897\n",
      "Epoch : 132\t Loss : 0.22246615588665009\n",
      "Epoch : 133\t Loss : 0.22222520411014557\n",
      "Epoch : 134\t Loss : 0.22197379171848297\n",
      "Epoch : 135\t Loss : 0.2218078225851059\n",
      "Epoch : 136\t Loss : 0.2216862589120865\n",
      "Epoch : 137\t Loss : 0.2213074415922165\n",
      "Epoch : 138\t Loss : 0.2210991084575653\n",
      "Epoch : 139\t Loss : 0.22074079513549805\n",
      "Epoch : 140\t Loss : 0.22047358751296997\n",
      "Epoch : 141\t Loss : 0.22018811106681824\n",
      "Epoch : 142\t Loss : 0.21971724927425385\n",
      "Epoch : 143\t Loss : 0.21946866810321808\n",
      "Epoch : 144\t Loss : 0.21914660930633545\n",
      "Epoch : 145\t Loss : 0.2188962996006012\n",
      "Epoch : 146\t Loss : 0.21877317130565643\n",
      "Epoch : 147\t Loss : 0.21867132186889648\n",
      "Epoch : 148\t Loss : 0.21839115023612976\n",
      "Epoch : 149\t Loss : 0.21798789501190186\n",
      "Epoch : 150\t Loss : 0.21795396506786346\n",
      "Epoch : 151\t Loss : 0.21756340563297272\n",
      "Epoch : 152\t Loss : 0.2173830270767212\n",
      "Epoch : 153\t Loss : 0.21698062121868134\n",
      "Epoch : 154\t Loss : 0.21705329418182373\n",
      "Epoch : 155\t Loss : 0.21673811972141266\n",
      "Epoch : 156\t Loss : 0.21642465889453888\n",
      "Epoch : 157\t Loss : 0.21610978245735168\n",
      "Epoch : 158\t Loss : 0.21584562957286835\n",
      "Epoch : 159\t Loss : 0.21579371392726898\n",
      "Epoch : 160\t Loss : 0.2153797745704651\n",
      "Epoch : 161\t Loss : 0.2155248373746872\n",
      "Epoch : 162\t Loss : 0.21541236340999603\n",
      "Epoch : 163\t Loss : 0.21578972041606903\n",
      "Epoch : 164\t Loss : 0.21578504145145416\n",
      "Epoch : 165\t Loss : 0.2156735360622406\n",
      "Epoch : 166\t Loss : 0.21564151346683502\n",
      "Epoch : 167\t Loss : 0.2154942899942398\n",
      "Epoch : 168\t Loss : 0.21535763144493103\n",
      "Epoch : 169\t Loss : 0.21498745679855347\n",
      "Epoch : 170\t Loss : 0.2146320641040802\n",
      "Epoch : 171\t Loss : 0.21452586352825165\n",
      "Epoch : 172\t Loss : 0.21424274146556854\n",
      "Epoch : 173\t Loss : 0.2142619788646698\n",
      "Epoch : 174\t Loss : 0.21413859724998474\n",
      "Epoch : 175\t Loss : 0.21384727954864502\n",
      "Epoch : 176\t Loss : 0.21358320116996765\n",
      "Epoch : 177\t Loss : 0.21336285769939423\n",
      "Epoch : 178\t Loss : 0.21309521794319153\n",
      "Epoch : 179\t Loss : 0.21337072551250458\n",
      "Epoch : 180\t Loss : 0.21316246688365936\n",
      "Epoch : 181\t Loss : 0.21287356317043304\n",
      "Epoch : 182\t Loss : 0.2126736342906952\n",
      "Epoch : 183\t Loss : 0.21247200667858124\n",
      "Epoch : 184\t Loss : 0.21217626333236694\n",
      "Epoch : 185\t Loss : 0.21208295226097107\n",
      "Epoch : 186\t Loss : 0.21203671395778656\n",
      "Epoch : 187\t Loss : 0.21201789379119873\n",
      "Epoch : 188\t Loss : 0.21186383068561554\n",
      "Epoch : 189\t Loss : 0.21162663400173187\n",
      "Epoch : 190\t Loss : 0.21162675321102142\n",
      "Epoch : 191\t Loss : 0.21199283003807068\n",
      "Epoch : 192\t Loss : 0.2116699069738388\n",
      "Epoch : 193\t Loss : 0.21144430339336395\n",
      "Epoch : 194\t Loss : 0.2113128900527954\n",
      "Epoch : 195\t Loss : 0.2111457735300064\n",
      "Epoch : 196\t Loss : 0.21096840500831604\n",
      "Epoch : 197\t Loss : 0.2110409140586853\n",
      "Epoch : 198\t Loss : 0.21088285744190216\n",
      "Epoch : 199\t Loss : 0.21075235307216644\n",
      "Epoch : 200\t Loss : 0.21085037291049957\n",
      "Epoch : 201\t Loss : 0.21073652803897858\n",
      "Epoch : 202\t Loss : 0.2105494737625122\n",
      "Epoch : 203\t Loss : 0.2103220820426941\n",
      "Epoch : 204\t Loss : 0.21016395092010498\n",
      "Epoch : 205\t Loss : 0.20998592674732208\n",
      "Epoch : 206\t Loss : 0.20976440608501434\n",
      "Epoch : 207\t Loss : 0.20998890697956085\n",
      "Epoch : 208\t Loss : 0.20997269451618195\n",
      "Epoch : 209\t Loss : 0.2098902463912964\n",
      "Epoch : 210\t Loss : 0.20984554290771484\n",
      "Epoch : 211\t Loss : 0.20971308648586273\n",
      "Epoch : 212\t Loss : 0.209442600607872\n",
      "Epoch : 213\t Loss : 0.20936144888401031\n",
      "Epoch : 214\t Loss : 0.20950698852539062\n",
      "Epoch : 215\t Loss : 0.20925316214561462\n",
      "Epoch : 216\t Loss : 0.20936359465122223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 217\t Loss : 0.20943550765514374\n",
      "Epoch : 218\t Loss : 0.20939595997333527\n",
      "Epoch : 219\t Loss : 0.20922604203224182\n",
      "Epoch : 220\t Loss : 0.20902642607688904\n",
      "Epoch : 221\t Loss : 0.2089705765247345\n",
      "Epoch : 222\t Loss : 0.20873039960861206\n",
      "Epoch : 223\t Loss : 0.2084929496049881\n",
      "Epoch : 224\t Loss : 0.20833244919776917\n",
      "Epoch : 225\t Loss : 0.20821478962898254\n",
      "Epoch : 226\t Loss : 0.20801104605197906\n",
      "Epoch : 227\t Loss : 0.2077987641096115\n",
      "Epoch : 228\t Loss : 0.20758239924907684\n",
      "Epoch : 229\t Loss : 0.207493394613266\n",
      "Epoch : 230\t Loss : 0.20737092196941376\n",
      "Epoch : 231\t Loss : 0.20735111832618713\n",
      "Epoch : 232\t Loss : 0.20713426172733307\n",
      "Epoch : 233\t Loss : 0.20688648521900177\n",
      "Epoch : 234\t Loss : 0.2069505751132965\n",
      "Epoch : 235\t Loss : 0.2069462537765503\n",
      "Epoch : 236\t Loss : 0.20697785913944244\n",
      "Epoch : 237\t Loss : 0.20710501074790955\n",
      "Epoch : 238\t Loss : 0.20696057379245758\n",
      "Epoch : 239\t Loss : 0.20700395107269287\n",
      "Epoch : 240\t Loss : 0.20684920251369476\n",
      "Epoch : 241\t Loss : 0.20697325468063354\n",
      "Epoch : 242\t Loss : 0.20692665874958038\n",
      "Epoch : 243\t Loss : 0.2067299336194992\n",
      "Epoch : 244\t Loss : 0.20662318170070648\n",
      "Epoch : 245\t Loss : 0.20656029880046844\n",
      "Epoch : 246\t Loss : 0.2066061943769455\n",
      "Epoch : 247\t Loss : 0.20662154257297516\n",
      "Epoch : 248\t Loss : 0.2064838856458664\n",
      "Epoch : 249\t Loss : 0.20642055571079254\n",
      "Epoch : 250\t Loss : 0.20617631077766418\n",
      "Epoch : 251\t Loss : 0.20620247721672058\n",
      "Epoch : 252\t Loss : 0.20608122646808624\n",
      "Epoch : 253\t Loss : 0.20593270659446716\n",
      "Epoch : 254\t Loss : 0.20581135153770447\n",
      "Epoch : 255\t Loss : 0.20578692853450775\n",
      "Epoch : 256\t Loss : 0.2057340294122696\n",
      "Epoch : 257\t Loss : 0.20550180971622467\n",
      "Epoch : 258\t Loss : 0.2053796350955963\n",
      "Epoch : 259\t Loss : 0.2052740752696991\n",
      "Epoch : 260\t Loss : 0.20515145361423492\n",
      "Epoch : 261\t Loss : 0.20496498048305511\n",
      "Epoch : 262\t Loss : 0.2047175019979477\n",
      "Epoch : 263\t Loss : 0.20452213287353516\n",
      "Epoch : 264\t Loss : 0.2043650895357132\n",
      "Epoch : 265\t Loss : 0.20419828593730927\n",
      "Epoch : 266\t Loss : 0.20401355624198914\n",
      "Epoch : 267\t Loss : 0.20391127467155457\n",
      "Epoch : 268\t Loss : 0.2037404179573059\n",
      "Epoch : 269\t Loss : 0.20352376997470856\n",
      "Epoch : 270\t Loss : 0.20329812169075012\n",
      "Epoch : 271\t Loss : 0.2031152993440628\n",
      "Epoch : 272\t Loss : 0.20308613777160645\n",
      "Epoch : 273\t Loss : 0.20294566452503204\n",
      "Epoch : 274\t Loss : 0.20285919308662415\n",
      "Epoch : 275\t Loss : 0.20274843275547028\n",
      "Epoch : 276\t Loss : 0.20264214277267456\n",
      "Epoch : 277\t Loss : 0.20262503623962402\n",
      "Epoch : 278\t Loss : 0.2025373876094818\n",
      "Epoch : 279\t Loss : 0.2023923099040985\n",
      "Epoch : 280\t Loss : 0.20229578018188477\n",
      "Epoch : 281\t Loss : 0.2021733969449997\n",
      "Epoch : 282\t Loss : 0.202043816447258\n",
      "Epoch : 283\t Loss : 0.20205740630626678\n",
      "Epoch : 284\t Loss : 0.20230190455913544\n",
      "Epoch : 285\t Loss : 0.2022717297077179\n",
      "Epoch : 286\t Loss : 0.2021670937538147\n",
      "Epoch : 287\t Loss : 0.2019956409931183\n",
      "Epoch : 288\t Loss : 0.20186452567577362\n",
      "Epoch : 289\t Loss : 0.20175018906593323\n",
      "Epoch : 290\t Loss : 0.20158831775188446\n",
      "Epoch : 291\t Loss : 0.20146678388118744\n",
      "Epoch : 292\t Loss : 0.2013905644416809\n",
      "Epoch : 293\t Loss : 0.20129583775997162\n",
      "Epoch : 294\t Loss : 0.2012198120355606\n",
      "Epoch : 295\t Loss : 0.20123879611492157\n",
      "Epoch : 296\t Loss : 0.20112143456935883\n",
      "Epoch : 297\t Loss : 0.20096567273139954\n",
      "Epoch : 298\t Loss : 0.2008509486913681\n",
      "Epoch : 299\t Loss : 0.2008749395608902\n",
      "Epoch : 300\t Loss : 0.2007761001586914\n",
      "Epoch : 301\t Loss : 0.20064230263233185\n",
      "Epoch : 302\t Loss : 0.20050746202468872\n",
      "Epoch : 303\t Loss : 0.20034901797771454\n",
      "Epoch : 304\t Loss : 0.20028826594352722\n",
      "Epoch : 305\t Loss : 0.2001079022884369\n",
      "Epoch : 306\t Loss : 0.1999005824327469\n",
      "Epoch : 307\t Loss : 0.19983527064323425\n",
      "Epoch : 308\t Loss : 0.19967855513095856\n",
      "Epoch : 309\t Loss : 0.19953987002372742\n",
      "Epoch : 310\t Loss : 0.19937729835510254\n",
      "Epoch : 311\t Loss : 0.19924645125865936\n",
      "Epoch : 312\t Loss : 0.19913192093372345\n",
      "Epoch : 313\t Loss : 0.19910332560539246\n",
      "Epoch : 314\t Loss : 0.1989758014678955\n",
      "Epoch : 315\t Loss : 0.19885699450969696\n",
      "Epoch : 316\t Loss : 0.19871856272220612\n",
      "Epoch : 317\t Loss : 0.1986585557460785\n",
      "Epoch : 318\t Loss : 0.19851860404014587\n",
      "Epoch : 319\t Loss : 0.19847597181797028\n",
      "Epoch : 320\t Loss : 0.19833363592624664\n",
      "Epoch : 321\t Loss : 0.19819258153438568\n",
      "Epoch : 322\t Loss : 0.1981080174446106\n",
      "Epoch : 323\t Loss : 0.19810588657855988\n",
      "Epoch : 324\t Loss : 0.19802911579608917\n",
      "Epoch : 325\t Loss : 0.19788651168346405\n",
      "Epoch : 326\t Loss : 0.19772018492221832\n",
      "Epoch : 327\t Loss : 0.197573721408844\n",
      "Epoch : 328\t Loss : 0.19745822250843048\n",
      "Epoch : 329\t Loss : 0.19735659658908844\n",
      "Epoch : 330\t Loss : 0.19731470942497253\n",
      "Epoch : 331\t Loss : 0.19724227488040924\n",
      "Epoch : 332\t Loss : 0.19714929163455963\n",
      "Epoch : 333\t Loss : 0.19705797731876373\n",
      "Epoch : 334\t Loss : 0.19697479903697968\n",
      "Epoch : 335\t Loss : 0.19685427844524384\n",
      "Epoch : 336\t Loss : 0.19675396382808685\n",
      "Epoch : 337\t Loss : 0.19665098190307617\n",
      "Epoch : 338\t Loss : 0.1964874565601349\n",
      "Epoch : 339\t Loss : 0.19631537795066833\n",
      "Epoch : 340\t Loss : 0.19618768990039825\n",
      "Epoch : 341\t Loss : 0.19607387483119965\n",
      "Epoch : 342\t Loss : 0.19594448804855347\n",
      "Epoch : 343\t Loss : 0.19587615132331848\n",
      "Epoch : 344\t Loss : 0.19574078917503357\n",
      "Epoch : 345\t Loss : 0.19561073184013367\n",
      "Epoch : 346\t Loss : 0.19551032781600952\n",
      "Epoch : 347\t Loss : 0.19538138806819916\n",
      "Epoch : 348\t Loss : 0.19537147879600525\n",
      "Epoch : 349\t Loss : 0.19534362852573395\n",
      "Epoch : 350\t Loss : 0.19534197449684143\n",
      "Epoch : 351\t Loss : 0.19521722197532654\n",
      "Epoch : 352\t Loss : 0.19512921571731567\n",
      "Epoch : 353\t Loss : 0.19497859477996826\n",
      "Epoch : 354\t Loss : 0.1948963850736618\n",
      "Epoch : 355\t Loss : 0.19473309814929962\n",
      "Epoch : 356\t Loss : 0.19462671875953674\n",
      "Epoch : 357\t Loss : 0.19460417330265045\n",
      "Epoch : 358\t Loss : 0.19448184967041016\n",
      "Epoch : 359\t Loss : 0.194361612200737\n",
      "Epoch : 360\t Loss : 0.1942581683397293\n",
      "Epoch : 361\t Loss : 0.1941523402929306\n",
      "Epoch : 362\t Loss : 0.19407005608081818\n",
      "Epoch : 363\t Loss : 0.19407379627227783\n",
      "Epoch : 364\t Loss : 0.1939399242401123\n",
      "Epoch : 365\t Loss : 0.19389717280864716\n",
      "Epoch : 366\t Loss : 0.19382892549037933\n",
      "Epoch : 367\t Loss : 0.19378656148910522\n",
      "Epoch : 368\t Loss : 0.1936303973197937\n",
      "Epoch : 369\t Loss : 0.19349457323551178\n",
      "Epoch : 370\t Loss : 0.1934039443731308\n",
      "Epoch : 371\t Loss : 0.19340695440769196\n",
      "Epoch : 372\t Loss : 0.1933428794145584\n",
      "Epoch : 373\t Loss : 0.19321413338184357\n",
      "Epoch : 374\t Loss : 0.19320139288902283\n",
      "Epoch : 375\t Loss : 0.1931406855583191\n",
      "Epoch : 376\t Loss : 0.19300924241542816\n",
      "Epoch : 377\t Loss : 0.19290168583393097\n",
      "Epoch : 378\t Loss : 0.1927604228258133\n",
      "Epoch : 379\t Loss : 0.19261886179447174\n",
      "Epoch : 380\t Loss : 0.19247345626354218\n",
      "Epoch : 381\t Loss : 0.1924007683992386\n",
      "Epoch : 382\t Loss : 0.19228368997573853\n",
      "Epoch : 383\t Loss : 0.19214774668216705\n",
      "Epoch : 384\t Loss : 0.19202229380607605\n",
      "Epoch : 385\t Loss : 0.19190296530723572\n",
      "Epoch : 386\t Loss : 0.19188767671585083\n",
      "Epoch : 387\t Loss : 0.19190117716789246\n",
      "Epoch : 388\t Loss : 0.19180111587047577\n",
      "Epoch : 389\t Loss : 0.19169677793979645\n",
      "Epoch : 390\t Loss : 0.19163203239440918\n",
      "Epoch : 391\t Loss : 0.19151020050048828\n",
      "Epoch : 392\t Loss : 0.19137270748615265\n",
      "Epoch : 393\t Loss : 0.19140774011611938\n",
      "Epoch : 394\t Loss : 0.1914156675338745\n",
      "Epoch : 395\t Loss : 0.19130656123161316\n",
      "Epoch : 396\t Loss : 0.19122089445590973\n",
      "Epoch : 397\t Loss : 0.19109277427196503\n",
      "Epoch : 398\t Loss : 0.19100232422351837\n",
      "Epoch : 399\t Loss : 0.1909579634666443\n",
      "Epoch : 400\t Loss : 0.19085828959941864\n",
      "Epoch : 401\t Loss : 0.19078660011291504\n",
      "Epoch : 402\t Loss : 0.1906987726688385\n",
      "Epoch : 403\t Loss : 0.19060175120830536\n",
      "Epoch : 404\t Loss : 0.19049659371376038\n",
      "Epoch : 405\t Loss : 0.1906159371137619\n",
      "Epoch : 406\t Loss : 0.19061751663684845\n",
      "Epoch : 407\t Loss : 0.19054429233074188\n",
      "Epoch : 408\t Loss : 0.19047461450099945\n",
      "Epoch : 409\t Loss : 0.19042004644870758\n",
      "Epoch : 410\t Loss : 0.19035521149635315\n",
      "Epoch : 411\t Loss : 0.19034777581691742\n",
      "Epoch : 412\t Loss : 0.1904723048210144\n",
      "Epoch : 413\t Loss : 0.19050860404968262\n",
      "Epoch : 414\t Loss : 0.19050240516662598\n",
      "Epoch : 415\t Loss : 0.19038505852222443\n",
      "Epoch : 416\t Loss : 0.19026640057563782\n",
      "Epoch : 417\t Loss : 0.19014579057693481\n",
      "Epoch : 418\t Loss : 0.1900360882282257\n",
      "Epoch : 419\t Loss : 0.18993854522705078\n",
      "Epoch : 420\t Loss : 0.18982423841953278\n",
      "Epoch : 421\t Loss : 0.1897617131471634\n",
      "Epoch : 422\t Loss : 0.18969017267227173\n",
      "Epoch : 423\t Loss : 0.18960896134376526\n",
      "Epoch : 424\t Loss : 0.18951626121997833\n",
      "Epoch : 425\t Loss : 0.18942739069461823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 426\t Loss : 0.18933789432048798\n",
      "Epoch : 427\t Loss : 0.1892506629228592\n",
      "Epoch : 428\t Loss : 0.18916364014148712\n",
      "Epoch : 429\t Loss : 0.18908391892910004\n",
      "Epoch : 430\t Loss : 0.18898117542266846\n",
      "Epoch : 431\t Loss : 0.18892988562583923\n",
      "Epoch : 432\t Loss : 0.1889338493347168\n",
      "Epoch : 433\t Loss : 0.1890299916267395\n",
      "Epoch : 434\t Loss : 0.18895797431468964\n",
      "Epoch : 435\t Loss : 0.18885259330272675\n",
      "Epoch : 436\t Loss : 0.18875622749328613\n",
      "Epoch : 437\t Loss : 0.1886506825685501\n",
      "Epoch : 438\t Loss : 0.18859611451625824\n",
      "Epoch : 439\t Loss : 0.18848484754562378\n",
      "Epoch : 440\t Loss : 0.18841440975666046\n",
      "Epoch : 441\t Loss : 0.18831288814544678\n",
      "Epoch : 442\t Loss : 0.18820790946483612\n",
      "Epoch : 443\t Loss : 0.1880849301815033\n",
      "Epoch : 444\t Loss : 0.18798205256462097\n",
      "Epoch : 445\t Loss : 0.1878637671470642\n",
      "Epoch : 446\t Loss : 0.1878775656223297\n",
      "Epoch : 447\t Loss : 0.18794706463813782\n",
      "Epoch : 448\t Loss : 0.1878661811351776\n",
      "Epoch : 449\t Loss : 0.18775007128715515\n",
      "Epoch : 450\t Loss : 0.18771147727966309\n",
      "Epoch : 451\t Loss : 0.18761205673217773\n",
      "Epoch : 452\t Loss : 0.18757958710193634\n",
      "Epoch : 453\t Loss : 0.18752789497375488\n",
      "Epoch : 454\t Loss : 0.18740926682949066\n",
      "Epoch : 455\t Loss : 0.18733549118041992\n",
      "Epoch : 456\t Loss : 0.1872721165418625\n",
      "Epoch : 457\t Loss : 0.18725179135799408\n",
      "Epoch : 458\t Loss : 0.18720100820064545\n",
      "Epoch : 459\t Loss : 0.18710877001285553\n",
      "Epoch : 460\t Loss : 0.18705962598323822\n",
      "Epoch : 461\t Loss : 0.1870402693748474\n",
      "Epoch : 462\t Loss : 0.18699157238006592\n",
      "Epoch : 463\t Loss : 0.18690206110477448\n",
      "Epoch : 464\t Loss : 0.18692319095134735\n",
      "Epoch : 465\t Loss : 0.18681222200393677\n",
      "Epoch : 466\t Loss : 0.18670400977134705\n",
      "Epoch : 467\t Loss : 0.186623677611351\n",
      "Epoch : 468\t Loss : 0.1866159588098526\n",
      "Epoch : 469\t Loss : 0.18660762906074524\n",
      "Epoch : 470\t Loss : 0.1865353286266327\n",
      "Epoch : 471\t Loss : 0.18648286163806915\n",
      "Epoch : 472\t Loss : 0.18639957904815674\n",
      "Epoch : 473\t Loss : 0.18633133172988892\n",
      "Epoch : 474\t Loss : 0.18625743687152863\n",
      "Epoch : 475\t Loss : 0.1863022893667221\n",
      "Epoch : 476\t Loss : 0.18628846108913422\n",
      "Epoch : 477\t Loss : 0.186285138130188\n",
      "Epoch : 478\t Loss : 0.18619650602340698\n",
      "Epoch : 479\t Loss : 0.1862056851387024\n",
      "Epoch : 480\t Loss : 0.18620029091835022\n",
      "Epoch : 481\t Loss : 0.18628168106079102\n",
      "Epoch : 482\t Loss : 0.18628694117069244\n",
      "Epoch : 483\t Loss : 0.18624114990234375\n",
      "Epoch : 484\t Loss : 0.1861584633588791\n",
      "Epoch : 485\t Loss : 0.18624885380268097\n",
      "Epoch : 486\t Loss : 0.186224102973938\n",
      "Epoch : 487\t Loss : 0.18617203831672668\n",
      "Epoch : 488\t Loss : 0.1860891431570053\n",
      "Epoch : 489\t Loss : 0.1860174536705017\n",
      "Epoch : 490\t Loss : 0.1859489232301712\n",
      "Epoch : 491\t Loss : 0.1859314888715744\n",
      "Epoch : 492\t Loss : 0.18583239614963531\n",
      "Epoch : 493\t Loss : 0.1857350766658783\n",
      "Epoch : 494\t Loss : 0.18573594093322754\n",
      "Epoch : 495\t Loss : 0.18576200306415558\n",
      "Epoch : 496\t Loss : 0.18576104938983917\n",
      "Epoch : 497\t Loss : 0.1857556402683258\n",
      "Epoch : 498\t Loss : 0.1857309490442276\n",
      "Epoch : 499\t Loss : 0.18570715188980103\n",
      "Epoch : 500\t Loss : 0.18561533093452454\n",
      "Epoch : 501\t Loss : 0.18558867275714874\n",
      "Epoch : 502\t Loss : 0.18553584814071655\n",
      "Epoch : 503\t Loss : 0.18549351394176483\n",
      "Epoch : 504\t Loss : 0.18546804785728455\n",
      "Epoch : 505\t Loss : 0.18540824949741364\n",
      "Epoch : 506\t Loss : 0.1854042112827301\n",
      "Epoch : 507\t Loss : 0.185325488448143\n",
      "Epoch : 508\t Loss : 0.1852814257144928\n",
      "Epoch : 509\t Loss : 0.18520449101924896\n",
      "Epoch : 510\t Loss : 0.18528467416763306\n",
      "Epoch : 511\t Loss : 0.1852695643901825\n",
      "Epoch : 512\t Loss : 0.18518690764904022\n",
      "Epoch : 513\t Loss : 0.18511122465133667\n",
      "Epoch : 514\t Loss : 0.18508335947990417\n",
      "Epoch : 515\t Loss : 0.1850263923406601\n",
      "Epoch : 516\t Loss : 0.18502657115459442\n",
      "Epoch : 517\t Loss : 0.18504831194877625\n",
      "Epoch : 518\t Loss : 0.1850420981645584\n",
      "Epoch : 519\t Loss : 0.18502973020076752\n",
      "Epoch : 520\t Loss : 0.1850104182958603\n",
      "Epoch : 521\t Loss : 0.1849687546491623\n",
      "Epoch : 522\t Loss : 0.18502269685268402\n",
      "Epoch : 523\t Loss : 0.18502943217754364\n",
      "Epoch : 524\t Loss : 0.18501611053943634\n",
      "Epoch : 525\t Loss : 0.1849638968706131\n",
      "Epoch : 526\t Loss : 0.18490631878376007\n",
      "Epoch : 527\t Loss : 0.18485914170742035\n",
      "Epoch : 528\t Loss : 0.18480907380580902\n",
      "Epoch : 529\t Loss : 0.18475830554962158\n",
      "Epoch : 530\t Loss : 0.18472369015216827\n",
      "Epoch : 531\t Loss : 0.18466611206531525\n",
      "Epoch : 532\t Loss : 0.18462349474430084\n",
      "Epoch : 533\t Loss : 0.18461303412914276\n",
      "Epoch : 534\t Loss : 0.18468618392944336\n",
      "Epoch : 535\t Loss : 0.18468807637691498\n",
      "Epoch : 536\t Loss : 0.18462911248207092\n",
      "Epoch : 537\t Loss : 0.18455052375793457\n",
      "Epoch : 538\t Loss : 0.18449020385742188\n",
      "Epoch : 539\t Loss : 0.1844172179698944\n",
      "Epoch : 540\t Loss : 0.184339702129364\n",
      "Epoch : 541\t Loss : 0.1842723786830902\n",
      "Epoch : 542\t Loss : 0.18421074748039246\n",
      "Epoch : 543\t Loss : 0.18413397669792175\n",
      "Epoch : 544\t Loss : 0.18407903611660004\n",
      "Epoch : 545\t Loss : 0.18402351438999176\n",
      "Epoch : 546\t Loss : 0.1839713305234909\n",
      "Epoch : 547\t Loss : 0.18397511541843414\n",
      "Epoch : 548\t Loss : 0.1838909387588501\n",
      "Epoch : 549\t Loss : 0.1838804930448532\n",
      "Epoch : 550\t Loss : 0.1838252693414688\n",
      "Epoch : 551\t Loss : 0.18377432227134705\n",
      "Epoch : 552\t Loss : 0.18375113606452942\n",
      "Epoch : 553\t Loss : 0.18371708691120148\n",
      "Epoch : 554\t Loss : 0.1836588829755783\n",
      "Epoch : 555\t Loss : 0.18364310264587402\n",
      "Epoch : 556\t Loss : 0.18371833860874176\n",
      "Epoch : 557\t Loss : 0.18366016447544098\n",
      "Epoch : 558\t Loss : 0.1836172491312027\n",
      "Epoch : 559\t Loss : 0.18356546759605408\n",
      "Epoch : 560\t Loss : 0.18350264430046082\n",
      "Epoch : 561\t Loss : 0.18348431587219238\n",
      "Epoch : 562\t Loss : 0.18343493342399597\n",
      "Epoch : 563\t Loss : 0.18335887789726257\n",
      "Epoch : 564\t Loss : 0.18330419063568115\n",
      "Epoch : 565\t Loss : 0.18322855234146118\n",
      "Epoch : 566\t Loss : 0.1832456886768341\n",
      "Epoch : 567\t Loss : 0.1832597255706787\n",
      "Epoch : 568\t Loss : 0.18318621814250946\n",
      "Epoch : 569\t Loss : 0.18311776220798492\n",
      "Epoch : 570\t Loss : 0.1830589771270752\n",
      "Epoch : 571\t Loss : 0.18301397562026978\n",
      "Epoch : 572\t Loss : 0.1829896718263626\n",
      "Epoch : 573\t Loss : 0.18299196660518646\n",
      "Epoch : 574\t Loss : 0.1829133927822113\n",
      "Epoch : 575\t Loss : 0.18286390602588654\n",
      "Epoch : 576\t Loss : 0.18284757435321808\n",
      "Epoch : 577\t Loss : 0.18277648091316223\n",
      "Epoch : 578\t Loss : 0.18269911408424377\n",
      "Epoch : 579\t Loss : 0.1826399266719818\n",
      "Epoch : 580\t Loss : 0.18259435892105103\n",
      "Epoch : 581\t Loss : 0.18254044651985168\n",
      "Epoch : 582\t Loss : 0.18250493705272675\n",
      "Epoch : 583\t Loss : 0.18242846429347992\n",
      "Epoch : 584\t Loss : 0.18240399658679962\n",
      "Epoch : 585\t Loss : 0.18238160014152527\n",
      "Epoch : 586\t Loss : 0.18244534730911255\n",
      "Epoch : 587\t Loss : 0.18239115178585052\n",
      "Epoch : 588\t Loss : 0.1823192834854126\n",
      "Epoch : 589\t Loss : 0.18225941061973572\n",
      "Epoch : 590\t Loss : 0.18228481709957123\n",
      "Epoch : 591\t Loss : 0.18220791220664978\n",
      "Epoch : 592\t Loss : 0.18219870328903198\n",
      "Epoch : 593\t Loss : 0.18212942779064178\n",
      "Epoch : 594\t Loss : 0.18207859992980957\n",
      "Epoch : 595\t Loss : 0.18201957643032074\n",
      "Epoch : 596\t Loss : 0.18194814026355743\n",
      "Epoch : 597\t Loss : 0.18191738426685333\n",
      "Epoch : 598\t Loss : 0.18185268342494965\n",
      "Epoch : 599\t Loss : 0.1817881315946579\n",
      "Epoch : 600\t Loss : 0.18174494802951813\n",
      "Epoch : 601\t Loss : 0.1817026436328888\n",
      "Epoch : 602\t Loss : 0.18163669109344482\n",
      "Epoch : 603\t Loss : 0.18160079419612885\n",
      "Epoch : 604\t Loss : 0.18152070045471191\n",
      "Epoch : 605\t Loss : 0.18146061897277832\n",
      "Epoch : 606\t Loss : 0.1813850700855255\n",
      "Epoch : 607\t Loss : 0.18133054673671722\n",
      "Epoch : 608\t Loss : 0.18126381933689117\n",
      "Epoch : 609\t Loss : 0.18123449385166168\n",
      "Epoch : 610\t Loss : 0.18128548562526703\n",
      "Epoch : 611\t Loss : 0.18123255670070648\n",
      "Epoch : 612\t Loss : 0.18120183050632477\n",
      "Epoch : 613\t Loss : 0.18117490410804749\n",
      "Epoch : 614\t Loss : 0.18114320933818817\n",
      "Epoch : 615\t Loss : 0.18119001388549805\n",
      "Epoch : 616\t Loss : 0.18123571574687958\n",
      "Epoch : 617\t Loss : 0.1811910718679428\n",
      "Epoch : 618\t Loss : 0.1811704933643341\n",
      "Epoch : 619\t Loss : 0.18116077780723572\n",
      "Epoch : 620\t Loss : 0.18117789924144745\n",
      "Epoch : 621\t Loss : 0.18116790056228638\n",
      "Epoch : 622\t Loss : 0.18111436069011688\n",
      "Epoch : 623\t Loss : 0.18104226887226105\n",
      "Epoch : 624\t Loss : 0.18099737167358398\n",
      "Epoch : 625\t Loss : 0.1809999942779541\n",
      "Epoch : 626\t Loss : 0.18095286190509796\n",
      "Epoch : 627\t Loss : 0.18089860677719116\n",
      "Epoch : 628\t Loss : 0.18087351322174072\n",
      "Epoch : 629\t Loss : 0.1808190494775772\n",
      "Epoch : 630\t Loss : 0.18082532286643982\n",
      "Epoch : 631\t Loss : 0.18079954385757446\n",
      "Epoch : 632\t Loss : 0.18076279759407043\n",
      "Epoch : 633\t Loss : 0.18076330423355103\n",
      "Epoch : 634\t Loss : 0.1807345300912857\n",
      "Epoch : 635\t Loss : 0.1807183474302292\n",
      "Epoch : 636\t Loss : 0.180674210190773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 637\t Loss : 0.18068107962608337\n",
      "Epoch : 638\t Loss : 0.18064530193805695\n",
      "Epoch : 639\t Loss : 0.18066371977329254\n",
      "Epoch : 640\t Loss : 0.18060478568077087\n",
      "Epoch : 641\t Loss : 0.18056970834732056\n",
      "Epoch : 642\t Loss : 0.1805478185415268\n",
      "Epoch : 643\t Loss : 0.1804877370595932\n",
      "Epoch : 644\t Loss : 0.18046145141124725\n",
      "Epoch : 645\t Loss : 0.18043120205402374\n",
      "Epoch : 646\t Loss : 0.18041278421878815\n",
      "Epoch : 647\t Loss : 0.1804439276456833\n",
      "Epoch : 648\t Loss : 0.18042226135730743\n",
      "Epoch : 649\t Loss : 0.18040385842323303\n",
      "Epoch : 650\t Loss : 0.180456280708313\n",
      "Epoch : 651\t Loss : 0.1805279552936554\n",
      "Epoch : 652\t Loss : 0.18055394291877747\n",
      "Epoch : 653\t Loss : 0.18055464327335358\n",
      "Epoch : 654\t Loss : 0.1805432140827179\n",
      "Epoch : 655\t Loss : 0.18052251636981964\n",
      "Epoch : 656\t Loss : 0.1804589480161667\n",
      "Epoch : 657\t Loss : 0.1804332584142685\n",
      "Epoch : 658\t Loss : 0.1803624927997589\n",
      "Epoch : 659\t Loss : 0.18037936091423035\n",
      "Epoch : 660\t Loss : 0.18041670322418213\n",
      "Epoch : 661\t Loss : 0.18038558959960938\n",
      "Epoch : 662\t Loss : 0.180330291390419\n",
      "Epoch : 663\t Loss : 0.18028289079666138\n",
      "Epoch : 664\t Loss : 0.18027737736701965\n",
      "Epoch : 665\t Loss : 0.1802317053079605\n",
      "Epoch : 666\t Loss : 0.18026261031627655\n",
      "Epoch : 667\t Loss : 0.18033868074417114\n",
      "Epoch : 668\t Loss : 0.1803760975599289\n",
      "Epoch : 669\t Loss : 0.18033549189567566\n",
      "Epoch : 670\t Loss : 0.18028758466243744\n",
      "Epoch : 671\t Loss : 0.18025638163089752\n",
      "Epoch : 672\t Loss : 0.18025712668895721\n",
      "Epoch : 673\t Loss : 0.18026798963546753\n",
      "Epoch : 674\t Loss : 0.1802673637866974\n",
      "Epoch : 675\t Loss : 0.1802120804786682\n",
      "Epoch : 676\t Loss : 0.18015126883983612\n",
      "Epoch : 677\t Loss : 0.18013255298137665\n",
      "Epoch : 678\t Loss : 0.180097296833992\n",
      "Epoch : 679\t Loss : 0.18005453050136566\n",
      "Epoch : 680\t Loss : 0.18002575635910034\n",
      "Epoch : 681\t Loss : 0.17997343838214874\n",
      "Epoch : 682\t Loss : 0.1799614578485489\n",
      "Epoch : 683\t Loss : 0.17992952466011047\n",
      "Epoch : 684\t Loss : 0.17989380657672882\n",
      "Epoch : 685\t Loss : 0.17985448241233826\n",
      "Epoch : 686\t Loss : 0.17991434037685394\n",
      "Epoch : 687\t Loss : 0.17994324862957\n",
      "Epoch : 688\t Loss : 0.17992053925991058\n",
      "Epoch : 689\t Loss : 0.17985419929027557\n",
      "Epoch : 690\t Loss : 0.1797943115234375\n",
      "Epoch : 691\t Loss : 0.1797608882188797\n",
      "Epoch : 692\t Loss : 0.17971882224082947\n",
      "Epoch : 693\t Loss : 0.1796586960554123\n",
      "Epoch : 694\t Loss : 0.17960362136363983\n",
      "Epoch : 695\t Loss : 0.17953966557979584\n",
      "Epoch : 696\t Loss : 0.179477259516716\n",
      "Epoch : 697\t Loss : 0.17943666875362396\n",
      "Epoch : 698\t Loss : 0.17937853932380676\n",
      "Epoch : 699\t Loss : 0.17934393882751465\n",
      "Epoch : 700\t Loss : 0.1792825311422348\n",
      "Epoch : 701\t Loss : 0.17924915254116058\n",
      "Epoch : 702\t Loss : 0.17920254170894623\n",
      "Epoch : 703\t Loss : 0.1791784167289734\n",
      "Epoch : 704\t Loss : 0.17921486496925354\n",
      "Epoch : 705\t Loss : 0.17916050553321838\n",
      "Epoch : 706\t Loss : 0.17911900579929352\n",
      "Epoch : 707\t Loss : 0.1790762096643448\n",
      "Epoch : 708\t Loss : 0.17909803986549377\n",
      "Epoch : 709\t Loss : 0.17904900014400482\n",
      "Epoch : 710\t Loss : 0.17913886904716492\n",
      "Epoch : 711\t Loss : 0.17910239100456238\n",
      "Epoch : 712\t Loss : 0.1790459156036377\n",
      "Epoch : 713\t Loss : 0.17903874814510345\n",
      "Epoch : 714\t Loss : 0.17903202772140503\n",
      "Epoch : 715\t Loss : 0.1789906919002533\n",
      "Epoch : 716\t Loss : 0.17894047498703003\n",
      "Epoch : 717\t Loss : 0.1788952797651291\n",
      "Epoch : 718\t Loss : 0.17887338995933533\n",
      "Epoch : 719\t Loss : 0.17895445227622986\n",
      "Epoch : 720\t Loss : 0.17905159294605255\n",
      "Epoch : 721\t Loss : 0.17907369136810303\n",
      "Epoch : 722\t Loss : 0.17903773486614227\n",
      "Epoch : 723\t Loss : 0.17905330657958984\n",
      "Epoch : 724\t Loss : 0.1790044903755188\n",
      "Epoch : 725\t Loss : 0.1789732724428177\n",
      "Epoch : 726\t Loss : 0.1789761185646057\n",
      "Epoch : 727\t Loss : 0.17894138395786285\n",
      "Epoch : 728\t Loss : 0.17888453602790833\n",
      "Epoch : 729\t Loss : 0.17884378135204315\n",
      "Epoch : 730\t Loss : 0.17884226143360138\n",
      "Epoch : 731\t Loss : 0.17881610989570618\n",
      "Epoch : 732\t Loss : 0.17876844108104706\n",
      "Epoch : 733\t Loss : 0.17872649431228638\n",
      "Epoch : 734\t Loss : 0.1787097454071045\n",
      "Epoch : 735\t Loss : 0.17868176102638245\n",
      "Epoch : 736\t Loss : 0.17864318192005157\n",
      "Epoch : 737\t Loss : 0.1786448210477829\n",
      "Epoch : 738\t Loss : 0.17866070568561554\n",
      "Epoch : 739\t Loss : 0.17874792218208313\n",
      "Epoch : 740\t Loss : 0.17881029844284058\n",
      "Epoch : 741\t Loss : 0.17879050970077515\n",
      "Epoch : 742\t Loss : 0.1787416636943817\n",
      "Epoch : 743\t Loss : 0.17868737876415253\n",
      "Epoch : 744\t Loss : 0.17868973314762115\n",
      "Epoch : 745\t Loss : 0.17874640226364136\n",
      "Epoch : 746\t Loss : 0.1787799745798111\n",
      "Epoch : 747\t Loss : 0.17878833413124084\n",
      "Epoch : 748\t Loss : 0.17875464260578156\n",
      "Epoch : 749\t Loss : 0.17877326905727386\n",
      "Epoch : 750\t Loss : 0.17874576151371002\n",
      "Epoch : 751\t Loss : 0.17872129380702972\n",
      "Epoch : 752\t Loss : 0.1787094622850418\n",
      "Epoch : 753\t Loss : 0.17866180837154388\n",
      "Epoch : 754\t Loss : 0.1786278486251831\n",
      "Epoch : 755\t Loss : 0.1785935014486313\n",
      "Epoch : 756\t Loss : 0.17855527997016907\n",
      "Epoch : 757\t Loss : 0.1785159707069397\n",
      "Epoch : 758\t Loss : 0.17847885191440582\n",
      "Epoch : 759\t Loss : 0.17846082150936127\n",
      "Epoch : 760\t Loss : 0.1784125119447708\n",
      "Epoch : 761\t Loss : 0.17836694419384003\n",
      "Epoch : 762\t Loss : 0.17833036184310913\n",
      "Epoch : 763\t Loss : 0.1782776266336441\n",
      "Epoch : 764\t Loss : 0.17826692759990692\n",
      "Epoch : 765\t Loss : 0.17827579379081726\n",
      "Epoch : 766\t Loss : 0.17824934422969818\n",
      "Epoch : 767\t Loss : 0.1781890094280243\n",
      "Epoch : 768\t Loss : 0.1782347559928894\n",
      "Epoch : 769\t Loss : 0.1782258301973343\n",
      "Epoch : 770\t Loss : 0.17818039655685425\n",
      "Epoch : 771\t Loss : 0.17820322513580322\n",
      "Epoch : 772\t Loss : 0.17821399867534637\n",
      "Epoch : 773\t Loss : 0.1781928539276123\n",
      "Epoch : 774\t Loss : 0.17823095619678497\n",
      "Epoch : 775\t Loss : 0.1782459020614624\n",
      "Epoch : 776\t Loss : 0.1783348023891449\n",
      "Epoch : 777\t Loss : 0.17837615311145782\n",
      "Epoch : 778\t Loss : 0.1783587783575058\n",
      "Epoch : 779\t Loss : 0.17829866707324982\n",
      "Epoch : 780\t Loss : 0.17825987935066223\n",
      "Epoch : 781\t Loss : 0.1782020777463913\n",
      "Epoch : 782\t Loss : 0.17818349599838257\n",
      "Epoch : 783\t Loss : 0.17814238369464874\n",
      "Epoch : 784\t Loss : 0.17811773717403412\n",
      "Epoch : 785\t Loss : 0.1780702918767929\n",
      "Epoch : 786\t Loss : 0.17803765833377838\n",
      "Epoch : 787\t Loss : 0.17799682915210724\n",
      "Epoch : 788\t Loss : 0.177964985370636\n",
      "Epoch : 789\t Loss : 0.17792834341526031\n",
      "Epoch : 790\t Loss : 0.17791788280010223\n",
      "Epoch : 791\t Loss : 0.17788535356521606\n",
      "Epoch : 792\t Loss : 0.17786653339862823\n",
      "Epoch : 793\t Loss : 0.17784613370895386\n",
      "Epoch : 794\t Loss : 0.17780013382434845\n",
      "Epoch : 795\t Loss : 0.17774350941181183\n",
      "Epoch : 796\t Loss : 0.17769990861415863\n",
      "Epoch : 797\t Loss : 0.17766132950782776\n",
      "Epoch : 798\t Loss : 0.17761173844337463\n",
      "Epoch : 799\t Loss : 0.17761382460594177\n",
      "Epoch : 800\t Loss : 0.17761243879795074\n",
      "Epoch : 801\t Loss : 0.17760337889194489\n",
      "Epoch : 802\t Loss : 0.17756985127925873\n",
      "Epoch : 803\t Loss : 0.17752940952777863\n",
      "Epoch : 804\t Loss : 0.1774890273809433\n",
      "Epoch : 805\t Loss : 0.17745527625083923\n",
      "Epoch : 806\t Loss : 0.17743779718875885\n",
      "Epoch : 807\t Loss : 0.1773906797170639\n",
      "Epoch : 808\t Loss : 0.17738042771816254\n",
      "Epoch : 809\t Loss : 0.17739352583885193\n",
      "Epoch : 810\t Loss : 0.17741546034812927\n",
      "Epoch : 811\t Loss : 0.17736278474330902\n",
      "Epoch : 812\t Loss : 0.17736166715621948\n",
      "Epoch : 813\t Loss : 0.17737449705600739\n",
      "Epoch : 814\t Loss : 0.17734704911708832\n",
      "Epoch : 815\t Loss : 0.17731553316116333\n",
      "Epoch : 816\t Loss : 0.1773429661989212\n",
      "Epoch : 817\t Loss : 0.17738574743270874\n",
      "Epoch : 818\t Loss : 0.17738491296768188\n",
      "Epoch : 819\t Loss : 0.17734737694263458\n",
      "Epoch : 820\t Loss : 0.17734099924564362\n",
      "Epoch : 821\t Loss : 0.17733104526996613\n",
      "Epoch : 822\t Loss : 0.17733179032802582\n",
      "Epoch : 823\t Loss : 0.17731265723705292\n",
      "Epoch : 824\t Loss : 0.17726394534111023\n",
      "Epoch : 825\t Loss : 0.1772003024816513\n",
      "Epoch : 826\t Loss : 0.1772432178258896\n",
      "Epoch : 827\t Loss : 0.17724570631980896\n",
      "Epoch : 828\t Loss : 0.17726841568946838\n",
      "Epoch : 829\t Loss : 0.17724552750587463\n",
      "Epoch : 830\t Loss : 0.17725063860416412\n",
      "Epoch : 831\t Loss : 0.1772552728652954\n",
      "Epoch : 832\t Loss : 0.17721427977085114\n",
      "Epoch : 833\t Loss : 0.17720448970794678\n",
      "Epoch : 834\t Loss : 0.17716367542743683\n",
      "Epoch : 835\t Loss : 0.17712779343128204\n",
      "Epoch : 836\t Loss : 0.1770889163017273\n",
      "Epoch : 837\t Loss : 0.17703573405742645\n",
      "Epoch : 838\t Loss : 0.17701025307178497\n",
      "Epoch : 839\t Loss : 0.1770341396331787\n",
      "Epoch : 840\t Loss : 0.17705504596233368\n",
      "Epoch : 841\t Loss : 0.17701131105422974\n",
      "Epoch : 842\t Loss : 0.1769782453775406\n",
      "Epoch : 843\t Loss : 0.1769288331270218\n",
      "Epoch : 844\t Loss : 0.17688900232315063\n",
      "Epoch : 845\t Loss : 0.17686037719249725\n",
      "Epoch : 846\t Loss : 0.17683139443397522\n",
      "Epoch : 847\t Loss : 0.17679812014102936\n",
      "Epoch : 848\t Loss : 0.17675627768039703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 849\t Loss : 0.17674724757671356\n",
      "Epoch : 850\t Loss : 0.1767440289258957\n",
      "Epoch : 851\t Loss : 0.17670327425003052\n",
      "Epoch : 852\t Loss : 0.17670565843582153\n",
      "Epoch : 853\t Loss : 0.17670048773288727\n",
      "Epoch : 854\t Loss : 0.176714226603508\n",
      "Epoch : 855\t Loss : 0.17667661607265472\n",
      "Epoch : 856\t Loss : 0.1766502410173416\n",
      "Epoch : 857\t Loss : 0.17662224173545837\n",
      "Epoch : 858\t Loss : 0.17659856379032135\n",
      "Epoch : 859\t Loss : 0.17658759653568268\n",
      "Epoch : 860\t Loss : 0.17674198746681213\n",
      "Epoch : 861\t Loss : 0.17688971757888794\n",
      "Epoch : 862\t Loss : 0.1768670678138733\n",
      "Epoch : 863\t Loss : 0.17685936391353607\n",
      "Epoch : 864\t Loss : 0.176853209733963\n",
      "Epoch : 865\t Loss : 0.17691431939601898\n",
      "Epoch : 866\t Loss : 0.17688138782978058\n",
      "Epoch : 867\t Loss : 0.17686942219734192\n",
      "Epoch : 868\t Loss : 0.1768529713153839\n",
      "Epoch : 869\t Loss : 0.1768105924129486\n",
      "Epoch : 870\t Loss : 0.1768011599779129\n",
      "Epoch : 871\t Loss : 0.17675167322158813\n",
      "Epoch : 872\t Loss : 0.1767248660326004\n",
      "Epoch : 873\t Loss : 0.1766829490661621\n",
      "Epoch : 874\t Loss : 0.17665554583072662\n",
      "Epoch : 875\t Loss : 0.17662179470062256\n",
      "Epoch : 876\t Loss : 0.17661873996257782\n",
      "Epoch : 877\t Loss : 0.17657232284545898\n",
      "Epoch : 878\t Loss : 0.1765730232000351\n",
      "Epoch : 879\t Loss : 0.17657023668289185\n",
      "Epoch : 880\t Loss : 0.1765323132276535\n",
      "Epoch : 881\t Loss : 0.17649123072624207\n",
      "Epoch : 882\t Loss : 0.17646054923534393\n",
      "Epoch : 883\t Loss : 0.17644834518432617\n",
      "Epoch : 884\t Loss : 0.17644807696342468\n",
      "Epoch : 885\t Loss : 0.1764240860939026\n",
      "Epoch : 886\t Loss : 0.17646223306655884\n",
      "Epoch : 887\t Loss : 0.17653033137321472\n",
      "Epoch : 888\t Loss : 0.1765051782131195\n",
      "Epoch : 889\t Loss : 0.1764829158782959\n",
      "Epoch : 890\t Loss : 0.1764800250530243\n",
      "Epoch : 891\t Loss : 0.17645055055618286\n",
      "Epoch : 892\t Loss : 0.1764182448387146\n",
      "Epoch : 893\t Loss : 0.17638945579528809\n",
      "Epoch : 894\t Loss : 0.1763550192117691\n",
      "Epoch : 895\t Loss : 0.1763169914484024\n",
      "Epoch : 896\t Loss : 0.17630039155483246\n",
      "Epoch : 897\t Loss : 0.17626842856407166\n",
      "Epoch : 898\t Loss : 0.17626196146011353\n",
      "Epoch : 899\t Loss : 0.1762760579586029\n",
      "Epoch : 900\t Loss : 0.17625612020492554\n",
      "Epoch : 901\t Loss : 0.17623992264270782\n",
      "Epoch : 902\t Loss : 0.17619909346103668\n",
      "Epoch : 903\t Loss : 0.17616674304008484\n",
      "Epoch : 904\t Loss : 0.1761648803949356\n",
      "Epoch : 905\t Loss : 0.17618879675865173\n",
      "Epoch : 906\t Loss : 0.17616060376167297\n",
      "Epoch : 907\t Loss : 0.1761379987001419\n",
      "Epoch : 908\t Loss : 0.1761201173067093\n",
      "Epoch : 909\t Loss : 0.17616136372089386\n",
      "Epoch : 910\t Loss : 0.17617648839950562\n",
      "Epoch : 911\t Loss : 0.1761527806520462\n",
      "Epoch : 912\t Loss : 0.17610567808151245\n",
      "Epoch : 913\t Loss : 0.17607982456684113\n",
      "Epoch : 914\t Loss : 0.17604497075080872\n",
      "Epoch : 915\t Loss : 0.1760517954826355\n",
      "Epoch : 916\t Loss : 0.1760997623205185\n",
      "Epoch : 917\t Loss : 0.1761256605386734\n",
      "Epoch : 918\t Loss : 0.1760810762643814\n",
      "Epoch : 919\t Loss : 0.17604373395442963\n",
      "Epoch : 920\t Loss : 0.17599475383758545\n",
      "Epoch : 921\t Loss : 0.17597408592700958\n",
      "Epoch : 922\t Loss : 0.17595182359218597\n",
      "Epoch : 923\t Loss : 0.17596934735774994\n",
      "Epoch : 924\t Loss : 0.17595349252223969\n",
      "Epoch : 925\t Loss : 0.17601941525936127\n",
      "Epoch : 926\t Loss : 0.17606733739376068\n",
      "Epoch : 927\t Loss : 0.17604973912239075\n",
      "Epoch : 928\t Loss : 0.17605459690093994\n",
      "Epoch : 929\t Loss : 0.17601777613162994\n",
      "Epoch : 930\t Loss : 0.17600668966770172\n",
      "Epoch : 931\t Loss : 0.17597027122974396\n",
      "Epoch : 932\t Loss : 0.17600806057453156\n",
      "Epoch : 933\t Loss : 0.17600078880786896\n",
      "Epoch : 934\t Loss : 0.17599454522132874\n",
      "Epoch : 935\t Loss : 0.17595568299293518\n",
      "Epoch : 936\t Loss : 0.17592713236808777\n",
      "Epoch : 937\t Loss : 0.17590560019016266\n",
      "Epoch : 938\t Loss : 0.1758880615234375\n",
      "Epoch : 939\t Loss : 0.17585834860801697\n",
      "Epoch : 940\t Loss : 0.1758824586868286\n",
      "Epoch : 941\t Loss : 0.17585313320159912\n",
      "Epoch : 942\t Loss : 0.17584627866744995\n",
      "Epoch : 943\t Loss : 0.17580834031105042\n",
      "Epoch : 944\t Loss : 0.17578479647636414\n",
      "Epoch : 945\t Loss : 0.17577412724494934\n",
      "Epoch : 946\t Loss : 0.17574211955070496\n",
      "Epoch : 947\t Loss : 0.17574436962604523\n",
      "Epoch : 948\t Loss : 0.17574025690555573\n",
      "Epoch : 949\t Loss : 0.17570596933364868\n",
      "Epoch : 950\t Loss : 0.1756710261106491\n",
      "Epoch : 951\t Loss : 0.17562679946422577\n",
      "Epoch : 952\t Loss : 0.1756141036748886\n",
      "Epoch : 953\t Loss : 0.1756121963262558\n",
      "Epoch : 954\t Loss : 0.17557334899902344\n",
      "Epoch : 955\t Loss : 0.17555716633796692\n",
      "Epoch : 956\t Loss : 0.1755346804857254\n",
      "Epoch : 957\t Loss : 0.17552612721920013\n",
      "Epoch : 958\t Loss : 0.1755291223526001\n",
      "Epoch : 959\t Loss : 0.17550455033779144\n",
      "Epoch : 960\t Loss : 0.17549660801887512\n",
      "Epoch : 961\t Loss : 0.17546346783638\n",
      "Epoch : 962\t Loss : 0.17543591558933258\n",
      "Epoch : 963\t Loss : 0.17544877529144287\n",
      "Epoch : 964\t Loss : 0.17544025182724\n",
      "Epoch : 965\t Loss : 0.17540086805820465\n",
      "Epoch : 966\t Loss : 0.17536309361457825\n",
      "Epoch : 967\t Loss : 0.17534463107585907\n",
      "Epoch : 968\t Loss : 0.17530417442321777\n",
      "Epoch : 969\t Loss : 0.17527008056640625\n",
      "Epoch : 970\t Loss : 0.17533673346042633\n",
      "Epoch : 971\t Loss : 0.1752987951040268\n",
      "Epoch : 972\t Loss : 0.17527003586292267\n",
      "Epoch : 973\t Loss : 0.17525289952754974\n",
      "Epoch : 974\t Loss : 0.17525771260261536\n",
      "Epoch : 975\t Loss : 0.17531001567840576\n",
      "Epoch : 976\t Loss : 0.17528535425662994\n",
      "Epoch : 977\t Loss : 0.17527329921722412\n",
      "Epoch : 978\t Loss : 0.17526045441627502\n",
      "Epoch : 979\t Loss : 0.17524182796478271\n",
      "Epoch : 980\t Loss : 0.175240159034729\n",
      "Epoch : 981\t Loss : 0.17523188889026642\n",
      "Epoch : 982\t Loss : 0.17522838711738586\n",
      "Epoch : 983\t Loss : 0.17522761225700378\n",
      "Epoch : 984\t Loss : 0.17538487911224365\n",
      "Epoch : 985\t Loss : 0.17542393505573273\n",
      "Epoch : 986\t Loss : 0.17538373172283173\n",
      "Epoch : 987\t Loss : 0.17535054683685303\n",
      "Epoch : 988\t Loss : 0.1753186136484146\n",
      "Epoch : 989\t Loss : 0.1753159910440445\n",
      "Epoch : 990\t Loss : 0.17529328167438507\n",
      "Epoch : 991\t Loss : 0.17527052760124207\n",
      "Epoch : 992\t Loss : 0.17526067793369293\n",
      "Epoch : 993\t Loss : 0.17528478801250458\n",
      "Epoch : 994\t Loss : 0.1752721071243286\n",
      "Epoch : 995\t Loss : 0.1752615123987198\n",
      "Epoch : 996\t Loss : 0.17523165047168732\n",
      "Epoch : 997\t Loss : 0.17523068189620972\n",
      "Epoch : 998\t Loss : 0.1752341389656067\n",
      "Epoch : 999\t Loss : 0.17520678043365479\n"
     ]
    }
   ],
   "source": [
    "#Model Training\n",
    "\n",
    "total_loss = 0\n",
    "n_batch =0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_train, y_train =batch\n",
    "        output = model(x_train.float())\n",
    "        loss = 0.5 * torch.norm(model.linear.weight.squeeze()) ** 2\n",
    "        loss += C * torch.clamp(1- y_train * output,min = 0).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss\n",
    "        n_batch += 1\n",
    "        \n",
    "    print(f\"Epoch : {epoch}\\t Loss : {total_loss / n_batch}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520de60",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcefa5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true_y, predicted_y):\n",
    "    return (true_y == predicted_y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30250727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.14827470481395721 \t Accuracy : 0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "n_batch = 0\n",
    "true_y = np.array([])\n",
    "predicted_y = np.array([])\n",
    "\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    x_test, y_test = batch\n",
    "    output = model(x_test.float())\n",
    "    loss = 0.5 * torch.norm(model.linear.weight.squeeze())**2\n",
    "    loss += C * torch.clamp(1 - y_test * output, min =0).mean()\n",
    "    \n",
    "    total_loss += loss\n",
    "    n_batch += 1\n",
    "    \n",
    "    true_y = np.append(true_y, y_test.numpy())\n",
    "    pred_y = 2 *(output >= 0) -1 \n",
    "    predicted_y = np.append(predicted_y, pred_y.numpy())\n",
    "\n",
    "print(f\"Loss : {total_loss / n_batch} \\t Accuracy : {accuracy(true_y, predicted_y)}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455ae1d",
   "metadata": {},
   "source": [
    "# Sklearn SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de2e5b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test =train_test_split(data['data'], 2 * data['target'] -1,test_size= 0.2, random_state = seed )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef1f6818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1d216bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, clf.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfe9610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
